{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "til1tLUUydky",
      "metadata": {
        "id": "til1tLUUydky"
      },
      "outputs": [],
      "source": [
        "MODELPATH='' # path to save the model\n",
        "MODELNAME=''\n",
        "TrainPATH='/Train.csv'\n",
        "TESTPATH='Test.csv'\n",
        "VIDEOPATH='' # add path to the video containing folder\n",
        "CNVSIZE=3900"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d27fa043",
      "metadata": {
        "id": "d27fa043"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "#from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils,models\n",
        "from PIL import Image\n",
        "import PIL\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        #transforms.ToTensor(),\n",
        "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        #transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "regress_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        #transforms.ToPILImage(),                        \n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        # transforms.RandomResizedCrop(224),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        #transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "34dfabbd",
      "metadata": {
        "id": "34dfabbd"
      },
      "outputs": [],
      "source": [
        "class StackedFramesDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file listing all the folders in the datasets\n",
        "            root_dir (string): Directory with all the csvs of images.        //ResnetData//images\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.markupcsv = pd.read_csv(csv_file)\n",
        "        self.markup= self.markupcsv[\"VideoFile\"].values\n",
        "        self.labels=self.markupcsv[\"HBLevel\"].values\n",
        "       \n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "       \n",
        "    def __len__(self):\n",
        "\n",
        "       return len(self.markup)\n",
        "    def __getitem__(self, idx):\n",
        "            \n",
        "        datapoint=None\n",
        "        flag=0\n",
        "        #\n",
        "        videoindex_to_read=idx\n",
        "        video_to_read=self.root_dir+self.markup[videoindex_to_read]\n",
        "        print(video_to_read)\n",
        "        capture = cv2.VideoCapture(video_to_read)  # open the video using OpenCV\n",
        "        #print(capture)\n",
        "        no_of_frame=int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        print(\"No of frames from the video\",no_of_frame )\n",
        "       \n",
        "        start=0 \n",
        "        end=1300 #specify number of frames to read from the video\n",
        "      \n",
        "        print(\"start:\",start,\" End:\",end)\n",
        "        frame = start  # keep track of which frame we are up to, starting from start\n",
        "        saved_count = 0  # a count of how many frames we have saved\n",
        "        framecount=0 # this will keep track of the depth of each image\n",
        "        capture.set(1, start)  # set the starting frame of the capture\n",
        "        while frame < end:\n",
        "\n",
        "            _, image = capture.read()\n",
        "            image=Image.fromarray((image).astype(np.uint8))\n",
        "            image=self.transform(image)  \n",
        "            \n",
        "            if(flag==0):\n",
        "            \n",
        "                flag=1;\n",
        "                datapoint=image\n",
        "            else:\n",
        "           \n",
        "            \n",
        "                datapoint=np.concatenate([datapoint,image],axis=0) #stack frames in Z-axis\n",
        "\n",
        "            frame=frame+1\n",
        "          \n",
        "        \n",
        "        \n",
        "        dp_Tensor=torch.Tensor(datapoint)\n",
        "        \n",
        "        #print(\"The output is below\")\n",
        "        print(datapoint.shape)\n",
        "        dp_Tensor=torch.Tensor(datapoint)\n",
        "        sample = (dp_Tensor, self.labels[videoindex_to_read])\n",
        "         \n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f91ae081",
      "metadata": {
        "id": "f91ae081"
      },
      "outputs": [],
      "source": [
        "epoch_dict={}\n",
        "import time\n",
        "from time import ctime\n",
        "import copy\n",
        "def regress_train_model(model, criterion, optimizer, scheduler, num_epochs=40):\n",
        "    since = time.time()\n",
        "    start_time = ctime()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        epoch_since = time.time()\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0            # Iterate over data.\n",
        "        for inputs, labels in data_loader:\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "         # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          with torch.set_grad_enabled(True):\n",
        "            #print(\":::::::::::::::::::Train:::::::::::::::::\")\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.float(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()               \n",
        "            # statistics\n",
        "            # print('{} loss item {:.4f} input size : {:d}', loss.item(), inputs.size(0))\n",
        "          running_loss += loss.item() * inputs.size(0) \n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / len(data_loader.dataset)\n",
        "        epoch_acc = 1 - epoch_loss\n",
        "\n",
        "        print(' Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                 epoch_loss, epoch_acc))\n",
        "        epoch_dict[epoch]=epoch_loss\n",
        "\n",
        "            # deep copy the model\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_model_wts, os.path.join(MODELPATH, MODELNAME+str(epoch) + '.pth'))\n",
        "\n",
        "\n",
        "        epoch_time_elapsed = time.time() - epoch_since\n",
        "        print('Current epoch completed in {:.0f}m {:.0f}s'.format(\n",
        "            epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    print(epoch_dict)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c67c072f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "c67c072f",
        "outputId": "d37eec4a-a83c-44af-d536-917df94e7639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset  120\n",
            "cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'inputs, curve_params = next(iter(data_loader))\\nprint(curve_params)\\nprint(inputs.shape)'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = StackedFramesDataset(csv_file=TrainPATH,\n",
        "                                    root_dir=VIDEOPATH,transform=regress_data_transforms['train'])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "data_loader = DataLoader(dataset,\n",
        "                  batch_size=1)\n",
        "net = models.resnet18(pretrained=False)\n",
        "net = net.cuda() if device else net\n",
        "net.conv1=nn.Conv2d(CNVSIZE, 64, kernel_size=7, stride=2, padding=3, bias=False) #update the first layer\n",
        "'''inputs, curve_params = next(iter(data_loader))\n",
        "print(curve_params)\n",
        "print(inputs.shape)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d832729",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d832729",
        "outputId": "4a2a0e1c-72a6-42bc-90d6-b0eb0a9af0d5",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "Epoch 0/49\n",
            "----------\n",
            "videoindex_to_read:   0\n",
            "2245-hareemfatima-june18.mp4\n",
            "/content/drive/MyDrive/pathwell/PATHWELLDATA/Pathwell-Data-Orignal/2245-hareemfatima-june18.mp4\n",
            "No of frames from the video 1898\n",
            "start: 0  End: 1300\n",
            "(3900, 224, 224)\n",
            "videoindex_to_read:   1\n",
            "1922-azanmqsood-1-june11.mp4\n",
            "/content/drive/MyDrive/pathwell/PATHWELLDATA/Pathwell-Data-Orignal/1922-azanmqsood-1-june11.mp4\n",
            "No of frames from the video 1893\n",
            "start: 0  End: 1300\n",
            "(3900, 224, 224)\n",
            "videoindex_to_read:   2\n",
            "2186-shafia-2.mp4\n",
            "/content/drive/MyDrive/pathwell/PATHWELLDATA/Pathwell-Data-Orignal/2186-shafia-2.mp4\n",
            "No of frames from the video 1898\n",
            "start: 0  End: 1300\n",
            "(3900, 224, 224)\n",
            "videoindex_to_read:   3\n",
            "1826-jumakhan-1-june9.mp4\n",
            "/content/drive/MyDrive/pathwell/PATHWELLDATA/Pathwell-Data-Orignal/1826-jumakhan-1-june9.mp4\n",
            "No of frames from the video 1872\n",
            "start: 0  End: 1300\n"
          ]
        }
      ],
      "source": [
        "#########################################################################################################################\n",
        "#########################################################################################################################\n",
        "# Put it all together\n",
        "#########################################################################################################################\n",
        "#########################################################################################################################\n",
        "num_ftrs = net.fc.in_features\n",
        "print(num_ftrs)\n",
        "\n",
        "net.fc = nn.Linear(num_ftrs, 1)\n",
        "net = net.to(device)\n",
        "model_name = os.path.join('Model', \"mymodal\" + '.pth')\n",
        "criterion = nn.MSELoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "net = regress_train_model(net, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db64fe26",
      "metadata": {
        "id": "db64fe26"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "'''\n",
        "#########################################################################################################################\n",
        "#########################################################################################################################\n",
        "# Test Phase\n",
        "#########################################################################################################################\n",
        "#########################################################################################################################\n",
        "'''\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    print(model)\n",
        "    model.eval()\n",
        "    use_gpu = False\n",
        "    dataset = StackedFramesDataset(csv_file='/home/humera/markup-females-test.csv',\n",
        "                                   root_dir='/home/humera/videos/',\n",
        "                                   transform=regress_data_transforms['train'])\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"In test the device is:\", device)\n",
        "    test_data_loader = DataLoader(dataset,\n",
        "                                  batch_size=1,\n",
        "                                  num_workers=1)\n",
        "    for data in test_data_loader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        # optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        print(\"________________________ Test Output__________________________\")\n",
        "        print(\"Actual Labels: \", labels )\n",
        "        print(\"Prediction Output\",outputs)\n",
        "        print(\"__________________________________________________\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0982c6d5",
      "metadata": {
        "id": "0982c6d5"
      },
      "outputs": [],
      "source": [
        "test(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7dc51a",
      "metadata": {
        "id": "ae7dc51a",
        "outputId": "a96bd7f8-8b05-4719-cc19-6b1aa5d49c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n",
            "dict_values([21.810815417426912, 5.2542299252989535, 5.006088390679317, 4.663215284126208, 4.6153725842013955, 4.47298484985881, 4.303402314181714, 3.776258765993749, 3.746988451217904, 3.739688528153826, 3.7308061735375837, 3.7227238016750883, 3.714580391698024, 3.7062709662405884, 3.6753155223148712, 3.5810883485657325, 3.5737637695582474, 3.5725822332589066, 3.5716211438617287, 3.5709064927171257, 3.569950125454103, 3.5596532076597214, 3.559588561382364, 3.5594840089187905, 3.5593656923841026, 3.5592579241184628, 3.559122079873786, 3.559027764507953, 3.5580222402863644, 3.558018160655218, 3.5580228740239845, 3.558000621769358, 3.557981078677318, 3.5579974149518154, 3.557995565673884, 3.557926693383385, 3.5579313997398403, 3.5579447395661297, 3.5579329450779102, 3.5579070345882107, 3.5579299394260433, 3.5579294720116783, 3.557912706013988, 3.5579161825863754, 3.55791431270978, 3.5579079523244324, 3.5579092929906704, 3.557905334102757, 3.5579172164640007, 3.5579179091488613, 3.557919265373665, 3.5579201197799515, 3.557921029846458, 3.557922280886594, 3.5579256051603485, 3.5579219072618904, 3.5579079183585502, 3.5579189607763992, 3.5579176832209614, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418, 3.557921065565418])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x= epoch_dict.keys()\n",
        "y=epoch_dict.values()\n",
        "print(x)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e975442f",
      "metadata": {
        "id": "e975442f",
        "outputId": "984abdcf-9fe6-43c2-9833-26a0a1e961cb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.plot(y, label=\"Model Loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch ')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('out.png', format = 'png', dpi=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe16fe20",
      "metadata": {
        "id": "fe16fe20"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
